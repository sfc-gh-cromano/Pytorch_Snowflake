{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "ghxycingbhhm2novn57f",
   "authorId": "4927852566776",
   "authorName": "CHASE",
   "authorEmail": "chase.romano@snowflake.com",
   "sessionId": "e46f140f-1a7d-470b-8f70-47f6fe8f6ba6",
   "lastEditTime": 1758133180362
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "summary"
   },
   "source": [
    "# Distributed PyTorch Training with Snowflake: CIFAR-10 Classification\n",
    "\n",
    "This notebook demonstrates how to run the PyTorch training framework tutorial using Snowflake's PyTorch Distributor for distributed training. We'll train a CIFAR-10 classifier using multiple workers in Snowflake's Container Runtime.\n",
    "\n",
    "## Overview\n",
    "- **Model**: CIFAR-10 CNN Classifier from the training framework\n",
    "- **Framework**: PyTorch Lightning adapted for Snowflake distributed training\n",
    "- **Distribution**: Snowflake PyTorchDistributor with multiple workers\n",
    "- **Data**: CIFAR-10 dataset distributed across workers using ShardedDataConnector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "_packages",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!uv pip install tfh_train-1.0.0-py3-none-any.whl lightning opencv-python-headless jaxtyping --system\n",
    "!uv pip install torch torchvision matplotlib pillow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "language": "python",
    "name": "_imports"
   },
   "outputs": [],
   "source": "# Snowflake imports\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\nfrom snowflake.ml.data.sharded_data_connector import ShardedDataConnector\nfrom snowflake.ml.modeling.distributors.pytorch import get_context\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import custom_model\n\n# PyTorch and related imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset, IterableDataset\nimport torchvision\nimport torchvision.transforms as transforms\nimport lightning as L\n\n# Standard library imports\nimport os\nimport sys\nimport functools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport base64\nimport io\nimport json\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tfh_train.model_zoo.cifar_clf.model import CifarClassifier\nfrom tfh_train.model_zoo.cifar_clf.data_module import CifarClassifierLightningDataModule\nfrom tfh_train.model_zoo.cifar_clf.model_module import CifarClassifierTraining\n\n# Get Snowflake session\nsession = get_active_session()\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Lightning version: {L.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# # Add the source directory to Python path\n# project_root = Path.cwd()\n# src_path = project_root / \"src\"\n# sys.path.append(str(src_path))\n\n# # Import our framework components\n# from tfh_train.model_zoo.cifar_clf.model import CifarClassifier\n# from tfh_train.model_zoo.cifar_clf.data_module import CifarClassifierLightningDataModule\n# from tfh_train.model_zoo.cifar_clf.model_module import CifarClassifierTraining\n\n# print(\"Training framework components imported successfully!\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "python",
    "name": "_dataprep"
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def prepare_cifar10_data():\n",
    "    \"\"\"Prepare CIFAR-10 data and convert to format suitable for Snowflake.\"\"\"\n",
    "    \n",
    "    # Download CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=None)  # No transform for raw data\n",
    "    \n",
    "    # Convert to format suitable for Snowflake (base64 encoded images)\n",
    "    data_records = []\n",
    "    \n",
    "    print(\"Converting CIFAR-10 data to Snowflake format...\")\n",
    "    for i, (image, label) in enumerate(trainset):\n",
    "        if i >= 5000:  # Limit dataset size for demo\n",
    "            break\n",
    "            \n",
    "        # Convert PIL image to base64\n",
    "        buffer = io.BytesIO()\n",
    "        image.save(buffer, format='PNG')\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        \n",
    "        data_records.append({\n",
    "            'IMAGE_ID': i,\n",
    "            'IMAGE_DATA': img_base64,\n",
    "            'LABEL': int(label),\n",
    "            'CLASS_NAME': cifar10_classes[label]\n",
    "        })\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i} images...\")\n",
    "    \n",
    "    # Create DataFrame and upload to Snowflake\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Create Snowflake DataFrame and table\n",
    "    snow_df = session.create_dataframe(df)\n",
    "    \n",
    "    # Create table\n",
    "    session.sql(\"\"\"\n",
    "        CREATE OR REPLACE TABLE CIFAR10_TRAINING_DATA (\n",
    "            IMAGE_ID NUMBER,\n",
    "            IMAGE_DATA VARCHAR(16777216),\n",
    "            LABEL NUMBER,\n",
    "            CLASS_NAME VARCHAR(50)\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Write data to table\n",
    "    snow_df.write.save_as_table(\"CIFAR10_TRAINING_DATA\", mode=\"overwrite\")\n",
    "    \n",
    "    print(f\"Successfully uploaded {len(data_records)} CIFAR-10 samples to Snowflake!\")\n",
    "    return len(data_records)\n",
    "\n",
    "# Prepare the data\n",
    "num_samples = prepare_cifar10_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "python",
    "name": "_view_data"
   },
   "outputs": [],
   "source": [
    "# Check the uploaded data\n",
    "result = session.sql(\"SELECT COUNT(*) as total_samples FROM CIFAR10_TRAINING_DATA\").collect()\n",
    "print(f\"Total samples in Snowflake: {result[0]['TOTAL_SAMPLES']}\")\n",
    "\n",
    "# Show sample data\n",
    "sample_data = session.table(\"CIFAR10_TRAINING_DATA\").limit(5).collect()\n",
    "for row in sample_data:\n",
    "    print(f\"Image ID: {row['IMAGE_ID']}, Label: {row['LABEL']}, Class: {row['CLASS_NAME']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "name": "_dist_train_desc",
    "collapsed": false
   },
   "source": "This function will be executed on each worker in the distributed training setup.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "name": "_distribute_train_func"
   },
   "outputs": [],
   "source": [
    "def distributed_cifar10_training():\n",
    "    \"\"\"Distributed training function for CIFAR-10 classification.\"\"\"\n",
    "    \n",
    "    # Import required libraries within the function for worker context\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torch.utils.data import DataLoader, IterableDataset\n",
    "    import torchvision.transforms as transforms\n",
    "    from PIL import Image\n",
    "    import base64\n",
    "    import io\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Add the source directory to Python path for worker\n",
    "    project_root = Path.cwd()\n",
    "    src_path = project_root / \"src\"\n",
    "    sys.path.append(str(src_path))\n",
    "    \n",
    "    # Import framework components\n",
    "    from tfh_train.model_zoo.cifar_clf.model import CifarClassifier\n",
    "    from snowflake.ml.modeling.distributors.pytorch import get_context\n",
    "    \n",
    "    # Get Snowflake context\n",
    "    context = get_context()\n",
    "    rank = context.get_rank()\n",
    "    world_size = context.get_world_size()\n",
    "    \n",
    "    # Initialize distributed training\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    print(f\"Worker Rank: {rank}, World Size: {world_size}\")\n",
    "    \n",
    "    # Custom dataset class for Snowflake data\n",
    "    class CIFAR10SnowflakeDataset(IterableDataset):\n",
    "        def __init__(self, source_dataset, transform_pipeline=None):\n",
    "            self.source_dataset = source_dataset\n",
    "            # Define transforms within the class to ensure they're available\n",
    "            if transform_pipeline is None:\n",
    "                self.transforms = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "            else:\n",
    "                self.transforms = transform_pipeline\n",
    "            \n",
    "            # Set a reasonable estimate for dataset length\n",
    "            # This will be used by DataLoader for progress tracking\n",
    "            self._length = 5000  # Based on our CIFAR-10 subset size\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self._length\n",
    "        \n",
    "        def __iter__(self):\n",
    "            for row in self.source_dataset:\n",
    "                # Decode base64 image\n",
    "                base64_image = row['IMAGE_DATA']\n",
    "                image_data = base64.b64decode(base64_image)\n",
    "                image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "                \n",
    "                # Apply transforms\n",
    "                if self.transforms:\n",
    "                    image = self.transforms(image)\n",
    "                \n",
    "                # Ensure label is a scalar integer (not tensor or array)\n",
    "                label = int(row['LABEL'])\n",
    "                yield image, label\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    with torch.cuda.device(rank) if torch.cuda.is_available() else torch.device(\"cpu\"):\n",
    "        # Initialize model (using the framework's CifarClassifier)\n",
    "        model = CifarClassifier()\n",
    "        model.to(device)\n",
    "        \n",
    "        # Wrap model with DDP\n",
    "        if torch.cuda.is_available():\n",
    "            model = DDP(model, device_ids=[rank])\n",
    "        else:\n",
    "            model = DDP(model)\n",
    "        \n",
    "        # Define loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Load data using ShardedDataConnector\n",
    "        dataset_map = context.get_dataset_map()\n",
    "        train_shard = dataset_map[\"train\"].get_shard().to_torch_dataset()\n",
    "        train_dataset = CIFAR10SnowflakeDataset(train_shard)\n",
    "        \n",
    "        # Get hyperparameters\n",
    "        hyper_params = context.get_hyper_params()\n",
    "        batch_size = int(hyper_params.get('batch_size', 32))\n",
    "        num_epochs = int(hyper_params.get('num_epochs', 5))\n",
    "        \n",
    "        # Custom collate function to ensure proper batching\n",
    "        def collate_fn(batch):\n",
    "            images, labels = zip(*batch)\n",
    "            # Stack images\n",
    "            images = torch.stack(images)\n",
    "            # Convert labels to tensor\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            return images, labels\n",
    "        \n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  # Shuffling handled by sharding\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                # Convert labels to proper tensor format\n",
    "                if isinstance(labels, (list, tuple)):\n",
    "                    labels = torch.tensor(labels, dtype=torch.long)\n",
    "                elif not isinstance(labels, torch.Tensor):\n",
    "                    labels = torch.tensor(labels, dtype=torch.long)\n",
    "                else:\n",
    "                    labels = labels.long()\n",
    "                \n",
    "                # Ensure labels are 1D\n",
    "                if labels.dim() > 1:\n",
    "                    labels = labels.squeeze()\n",
    "                \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Debug print for first batch\n",
    "                if batch_idx == 0 and epoch == 0:\n",
    "                    print(f\"[Rank {rank}] Debug - Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "                    print(f\"[Rank {rank}] Debug - Labels dtype: {labels.dtype}, Labels sample: {labels[:5]}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                running_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"[Rank {rank}] Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                          f\"Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Epoch statistics\n",
    "            if batch_idx > 0:  # Avoid division by zero\n",
    "                epoch_loss = running_loss / (batch_idx + 1)\n",
    "                epoch_acc = 100 * running_correct / total_samples\n",
    "                print(f\"[Rank {rank}] Epoch [{epoch+1}/{num_epochs}] completed. \"\n",
    "                      f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, \"\n",
    "                      f\"Processed {batch_idx + 1} batches, {total_samples} samples\")\n",
    "        \n",
    "        # Save model (only rank 0)\n",
    "        if rank == 0:\n",
    "            model_path = \"/tmp/cifar10_model.pt\"\n",
    "            if hasattr(model, 'module'):\n",
    "                torch.save(model.module.state_dict(), model_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        print(f\"[Rank {rank}] Training completed successfully!\")\n",
    "\n",
    "print(\"Distributed training function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "language": "python",
    "name": "_configuration"
   },
   "outputs": [],
   "source": "# Create data connector for the training data\ndf = session.table(\"CIFAR10_TRAINING_DATA\")\ntrain_data = ShardedDataConnector.from_dataframe(df)\n\n# Configure PyTorch Distributor\npytorch_trainer = PyTorchDistributor(\n    train_func=distributed_cifar10_training,\n    scaling_config=PyTorchScalingConfig(\n        num_nodes=1,\n        num_workers_per_node=1,  # Adjust based on available resources\n        resource_requirements_per_worker=WorkerResourceConfig(\n            num_cpus=10, \n            num_gpus=2 if torch.cuda.is_available() else 0\n        )\n    )\n)"
  },
  {
   "cell_type": "code",
   "id": "fcd58126-b8b5-4e01-b168-c96d849e3cc0",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "!nvidia-smi",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "language": "python",
    "name": "_training"
   },
   "outputs": [],
   "source": [
    "# Run distributed training\n",
    "print(\"Starting distributed CIFAR-10 training...\")\n",
    "\n",
    "training_result = pytorch_trainer.run(\n",
    "    dataset_map={\"train\": train_data},\n",
    "    hyper_params={\n",
    "        \"batch_size\": \"32\",\n",
    "        \"num_epochs\": \"5\",\n",
    "        \"learning_rate\": \"0.001\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Distributed training completed!\")\n",
    "print(f\"Training result: {training_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "language": "python",
    "name": "_Model_Registry"
   },
   "outputs": [],
   "source": [
    "# Define custom model wrapper for Snowflake Model Registry\n",
    "class CIFAR10ClassificationModel(custom_model.CustomModel):\n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        self.classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                       'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    def decode_and_transform_image(self, base64_image):\n",
    "        \"\"\"Decode base64 image and apply transforms.\"\"\"\n",
    "        import base64\n",
    "        import io\n",
    "        from PIL import Image\n",
    "        import torchvision.transforms as transforms\n",
    "        \n",
    "        image_data = base64.b64decode(base64_image)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        return transform(image)\n",
    "    \n",
    "    @custom_model.inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Predict CIFAR-10 classes from base64 encoded images.\"\"\"\n",
    "        \n",
    "        # Import required modules within the method\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get model first to determine device\n",
    "        model = self.context.model_ref(\"cifar10_classifier\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Determine the device the model is on\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"Model device: {device}\")\n",
    "        \n",
    "        # Process input images\n",
    "        processed_images = []\n",
    "        for base64_img in input_df['IMAGE_DATA']:\n",
    "            img_tensor = self.decode_and_transform_image(base64_img)\n",
    "            processed_images.append(img_tensor)\n",
    "        \n",
    "        # Stack into batch and move to same device as model\n",
    "        batch = torch.stack(processed_images)\n",
    "        batch = batch.to(device)  # Ensure input is on same device as model\n",
    "        print(f\"Input batch device: {batch.device}, shape: {batch.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move results back to CPU for processing\n",
    "        probabilities = probabilities.cpu()\n",
    "        predicted_classes = predicted_classes.cpu()\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i in range(len(predicted_classes)):\n",
    "            pred_class = predicted_classes[i].item()\n",
    "            confidence = probabilities[i][pred_class].item()\n",
    "            \n",
    "            results.append({\n",
    "                'predicted_class': pred_class,\n",
    "                'predicted_label': self.classes[pred_class],\n",
    "                'confidence': confidence,\n",
    "                'probabilities': probabilities[i].tolist()\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "print(\"Custom model wrapper defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "language": "python",
    "name": "_Load_Model"
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "def load_trained_model(model_path='/tmp/cifar10_model.pt'):\n",
    "    \"\"\"Load the trained CIFAR-10 model.\"\"\"\n",
    "    # Ensure imports are available\n",
    "    import torch\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Add source path\n",
    "    project_root = Path.cwd()\n",
    "    src_path = project_root / \"src\"\n",
    "    if str(src_path) not in sys.path:\n",
    "        sys.path.append(str(src_path))\n",
    "    \n",
    "    # Import model class\n",
    "    from tfh_train.model_zoo.cifar_clf.model import CifarClassifier\n",
    "    \n",
    "    # Create model instance\n",
    "    model = CifarClassifier()\n",
    "    \n",
    "    try:\n",
    "        # Try to load the state dict, force to CPU for inference service compatibility\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file not found at {model_path}. Using untrained model for demo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}. Using untrained model for demo.\")\n",
    "    \n",
    "    # Ensure model is on CPU for inference service\n",
    "    model = model.cpu()\n",
    "    model.eval()\n",
    "    print(f\"Model moved to CPU for inference service compatibility\")\n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "trained_model = load_trained_model()\n",
    "\n",
    "# Create sample input for model signature\n",
    "sample_data = session.table(\"CIFAR10_TRAINING_DATA\").limit(1).to_pandas()\n",
    "sample_input = session.create_dataframe(sample_data[['IMAGE_DATA']])\n",
    "\n",
    "# Create model instance with context\n",
    "cifar10_model = CIFAR10ClassificationModel(\n",
    "    context=custom_model.ModelContext(\n",
    "        models={'cifar10_classifier': trained_model}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Model prepared for registration!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "language": "python",
    "name": "_Log_Model"
   },
   "outputs": [],
   "source": "# Initialize Model Registry\nml_registry = Registry(session=session)\n\n# Register the model\ntry:\n    model_version = ml_registry.log_model(\n        cifar10_model,\n        model_name=\"CIFAR10_CLASSIFIER\",\n        version_name=\"v2_distributed\",\n        sample_input_data=sample_input,\n        target_platforms = ['SNOWPARK_CONTAINER_SERVICES'],\n        conda_dependencies=[\"pytorch\", \"torchvision\", \"pillow\", \"numpy\"],\n        options={\n            \"embed_local_ml_library\": True,\n            \"relax\": True\n        }\n    )\n    \n    print(f\"Model registered successfully!\")\n    print(f\"Model name: CIFAR10_CLASSIFIER\")\n    print(f\"Version: v1_distributed\")\n    print(f\"Model version object: {model_version}\")\n    \nexcept Exception as e:\n    print(f\"Error registering model: {e}\")\n    print(\"This might be due to missing model file or registry permissions.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "language": "python",
    "name": "_Local_inference"
   },
   "outputs": [],
   "source": [
    "# Direct inference using the trained model in notebook environment\n",
    "try:\n",
    "    # Get test data\n",
    "    test_data = session.table(\"CIFAR10_TRAINING_DATA\").limit(3).to_pandas()\n",
    "    \n",
    "    print(\"Running direct inference on test samples...\")\n",
    "    \n",
    "    # Use the trained model directly\n",
    "    trained_model.eval()\n",
    "    \n",
    "    # Process test images\n",
    "    results = []\n",
    "    for i, row in test_data.iterrows():\n",
    "        # Decode and transform image\n",
    "        base64_image = row['IMAGE_DATA']\n",
    "        image_data = base64.b64decode(base64_image)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = trained_model(image_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        results.append({\n",
    "            'actual_label': row['CLASS_NAME'],\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_label': cifar10_classes[predicted_class],\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    print(\"\\nDirect Inference Results:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Actual: {result['actual_label']}\")\n",
    "        print(f\"  Predicted: {result['predicted_label']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during direct inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ]
}